# ğŸ§ª Worksheet Generation Testing Guide

This guide provides comprehensive testing procedures to verify that worksheet generation correctly reflects selected pedagogical options (tools, strategies, representation types, etc.).

## ğŸ¯ Overview

The testing system provides multiple verification layers:
1. **Debug Prompt Endpoint** - Inspect exact prompts sent to Claude AI
2. **Enhanced Logging** - Server-side parameter and response analysis  
3. **PDF Selection Summary** - Visual verification on generated worksheets
4. **Compliance Checking** - Automated analysis of worksheet content
5. **Test Scripts** - Automated validation workflows

---

## ğŸ”§ Testing Tools

### 1. Debug Prompt Endpoint (`/api/debug/prompt`)
**Purpose**: Generate and analyze prompts without calling Claude AI

**Usage**:
```bash
curl -X POST http://localhost:3001/api/debug/prompt \
-H "Content-Type: application/json" \
-d '{
  "gradeLevel": 3,
  "topic": "multiplication-basic", 
  "difficulty": "medium",
  "problemCount": 5,
  "visualTheme": "animals",
  "mathematicalTools": ["arrays", "area_models"],
  "problemSolvingStrategy": "draw_picture",
  "scaffoldingLevel": "guided",
  "representationType": "concrete",
  "includeThinkingPrompts": true
}'
```

**What to verify**:
- âœ… `analysis.hasToolInstructions` = true
- âœ… `analysis.hasStrategyInstructions` = true  
- âœ… `analysis.containsMandatory` = true
- âœ… `analysis.containsCritical` = true
- âœ… Prompt preview contains your selected tools/strategies

### 2. Enhanced Test Route (`/api/test-claude`)
**Purpose**: Test full generation pipeline with Claude AI

**Usage**:
```bash
curl -X POST http://localhost:3001/api/test-claude \
-H "Content-Type: application/json" \
-d '{
  "gradeLevel": 2,
  "topic": "addition-basic",
  "mathematicalTools": ["ten_frames"],
  "representationType": "pictorial"
}'
```

**What to verify**:
- âœ… Parameters correctly processed
- âœ… Prompt analysis shows expected instructions
- âœ… Claude response contains JSON worksheet
- âœ… Generated problems reflect selections

### 3. Server Logs
**Purpose**: Real-time monitoring of generation process

**How to view**:
```bash
# If using npm run dev
# Check terminal output for detailed logs

# Key log entries to look for:
# [Generate] Received parameters: {...}
# [Generate] Prompt analysis: {...}  
# [Generate] Compliance check results: {...}
```

**What to verify**:
- âœ… Parameters received correctly
- âœ… Prompt includes tool/strategy instructions
- âœ… Compliance score > 80
- âœ… No major violations detected

---

## ğŸ® Test Scenarios

### Scenario 1: Concrete Representation Test
**Goal**: Verify "Concrete" mode produces only physical manipulative problems

**Setup**:
```json
{
  "gradeLevel": 2,
  "topic": "addition-basic",
  "representationType": "concrete",
  "mathematicalTools": ["base_ten_blocks"],
  "problemCount": 5
}
```

**Expected Results**:
- âœ… All problems reference physical objects (blocks, bears, toys)
- âœ… NO abstract computation (e.g., "5 + 3 = ?")
- âœ… PDF header shows "Representation: concrete"
- âœ… Compliance score â‰¥ 90

**Red Flags**:
- âŒ Problems like "What is 7 + 4?"
- âŒ Abstract number sentences without context
- âŒ Compliance violations for representation

### Scenario 2: Mathematical Tools Verification
**Goal**: Verify selected tools are actually used in problems

**Setup**:
```json
{
  "gradeLevel": 3,
  "topic": "multiplication-basic", 
  "mathematicalTools": ["arrays", "area_models"],
  "problemCount": 5
}
```

**Expected Results**:
- âœ… 60%+ of problems mention "arrays", "rows and columns", or "area model"
- âœ… Problems designed to use these specific tools
- âœ… PDF header shows "Tools: arrays, area_models"
- âœ… Compliance tool detection â‰¥ 60%

**Red Flags**:
- âŒ Generic multiplication problems without tool references
- âŒ Tool compliance < 60%
- âŒ No mention of selected tools in problems

### Scenario 3: Problem-Solving Strategy Test
**Goal**: Verify strategy focus in problem design

**Setup**:
```json
{
  "gradeLevel": 4,
  "topic": "word-problems-multistep",
  "problemSolvingStrategy": "draw_picture", 
  "scaffoldingLevel": "guided",
  "problemCount": 5
}
```

**Expected Results**:
- âœ… 40%+ of problems include "draw", "picture", "diagram" instructions
- âœ… Problems designed to benefit from visual representation
- âœ… PDF header shows "Strategy: draw picture"
- âœ… Strategy compliance â‰¥ 40%

**Red Flags**:
- âŒ Pure computation problems without strategy guidance
- âŒ Strategy compliance < 40%
- âŒ No explicit strategy instructions

### Scenario 4: Scaffolding Level Verification
**Goal**: Verify support level affects problem structure

**Setup - Independent**:
```json
{
  "scaffoldingLevel": "none",
  "gradeLevel": 5,
  "topic": "division-basic"
}
```

**Expected Results**:
- âœ… Clean, direct problem statements
- âœ… NO hints, prompts, or guidance
- âœ… PDF header shows "Support: none"

**Setup - Heavy Support**:
```json
{
  "scaffoldingLevel": "heavy",
  "gradeLevel": 3, 
  "topic": "fractions-basic"
}
```

**Expected Results**:
- âœ… Step-by-step guidance in problems
- âœ… Hints like "First, try...", "Remember that..."
- âœ… Multi-step problems broken down
- âœ… PDF header shows "Support: heavy"

---

## ğŸ” Quick Verification Checklist

### Before Generation:
- [ ] Server running on correct port (3001)
- [ ] Environment variables configured (ANTHROPIC_API_KEY, etc.)
- [ ] Database connection working

### After Selecting Options:
- [ ] Use debug endpoint to verify prompt contains your selections
- [ ] Check browser Network tab for correct API request payload
- [ ] Verify no console errors in browser

### During Generation:
- [ ] Monitor server logs for parameter reception
- [ ] Check compliance analysis in logs
- [ ] Verify no generation errors

### After PDF Generation:
- [ ] PDF header shows selected options correctly
- [ ] Problems reflect chosen representation type
- [ ] Tools/strategies evident in problem content
- [ ] Compliance score documented in logs

---

## ğŸš¨ Troubleshooting Common Issues

### Issue: "Prompt doesn't include my selections"
**Solution**:
1. Use `/api/debug/prompt` to inspect exact prompt
2. Check if parameters passed correctly in request
3. Verify tool/strategy functions are called properly

### Issue: "PDF shows wrong selections" 
**Solution**:
1. Check API route passes `selections` to PDF generator
2. Verify PDF generator receives all parameters
3. Look for TypeScript type mismatches

### Issue: "Generated problems ignore representation type"
**Solution**:
1. Check compliance score in server logs
2. Verify prompt contains MANDATORY directives
3. Consider strengthening prompt language further

### Issue: "Compliance score always low"
**Solution**:
1. Review compliance checker keyword matching
2. Adjust detection algorithms for your specific use case
3. Check if Claude AI response format changed

---

## ğŸ§ª Automated Test Script

Run the provided test script for comprehensive validation:

```bash
node test-debug.js
```

This will:
- âœ… Test debug prompt endpoint
- âœ… Test enhanced test route with all parameters
- âœ… Verify prompt analysis functionality
- âœ… Show detailed results and next steps

---

## ğŸ“Š Success Metrics

### High Priority (Must Pass):
- âœ… Compliance score â‰¥ 80
- âœ… Representation type enforced correctly
- âœ… PDF selection summary displays
- âœ… No critical generation errors

### Medium Priority (Should Pass):
- âœ… Tool compliance â‰¥ 60%
- âœ… Strategy compliance â‰¥ 40%
- âœ… Scaffolding level appropriate
- âœ… Thinking prompts when enabled

### Low Priority (Nice to Have):
- âœ… Compliance score â‰¥ 95
- âœ… All problems use selected tools
- âœ… Perfect strategy implementation
- âœ… Advanced scaffolding features

---

## ğŸ“ Advanced Testing

### Custom Compliance Rules
Modify `/lib/utils/compliance-checker.ts` to add:
- Subject-specific tool detection
- Grade-level appropriate complexity
- Custom keyword patterns
- Institution-specific requirements

### Prompt Optimization
Use debug endpoint to:
- Test different prompt structures
- A/B test directive language
- Optimize for specific Claude AI versions
- Fine-tune compliance rates

### Performance Testing
- Test with different problem counts (5-20)
- Verify generation time remains reasonable
- Check memory usage with complex selections
- Validate PDF generation performance

---

## ğŸ† Quality Assurance

### Manual Review Process:
1. **Generate 5 test worksheets** with different option combinations
2. **Review each PDF** for selection summary accuracy
3. **Read all problems** to verify pedagogical alignment
4. **Check compliance reports** in server logs
5. **Document any discrepancies** for improvement

### Regression Testing:
- Test major option combinations monthly
- Verify core functionality after updates
- Maintain test worksheet library for comparison
- Monitor compliance score trends over time

---

*This testing guide ensures your worksheet generation accurately reflects pedagogical selections and provides teachers with the customized learning materials they expect.*